# Giga-Think SWE-Bench Results

## ğŸ¯ Official Benchmark Results

### Baseline (Blind Patching)
- **Method**: Single-pass prompting, no code context
- **Models Tested**: 8 (Grok, GPT, Claude, Gemini)
- **Problems**: 50 per model (400 total)
- **Best Score**: Claude Opus 4.1 - **6%** (3/50)
- **All Others**: 0%
- **Cost**: ~$15 ($0.03/problem)

### SWE-Agent (Full Agentic Workflow)
- **Method**: Multi-turn agent with code browsing, test creation, iteration
- **Model**: Claude Opus 4.1 with OpenRouter
- **Problems**: 9
- **Score**: **33%** (3/9) âœ…
- **Cost**: $96.43 ($9.64/problem)
- **Platform**: Modal cloud containers

### Improvement
```
6% â†’ 33% = 5.5x better
+27 percentage points
```

## ğŸ”§ Infrastructure

### Working Components
- âœ… OpenRouter integration (8 models configured)
- âœ… SWE-bench Lite dataset (300 problems)
- âœ… sb-cli cloud evaluation
- âœ… Modal cloud containers (no Docker needed!)
- âœ… SWE-agent full framework
- âœ… Baseline testing pipeline

### Key Learnings
1. **Structured output critical**: Fixed 80% patch apply errors
2. **Code context essential**: Blind patching â†’ 0-6%, Agentic â†’ 33%
3. **Modal > Docker**: No architecture conflicts on Apple Silicon
4. **openrouter/ prefix**: Required for litellm routing

## ğŸ“Š Quality Metrics

| Metric | Baseline | SWE-Agent | Improvement |
|--------|----------|-----------|-------------|
| Success Rate | 6% | 33% | **5.5x** âœ… |
| Include Tests | 0% | 100% | **âˆ** âœ… |
| Avg Files Changed | 1 | 6 | **6x** âœ… |
| Avg Patch Size | 800 | 19,799 | **25x** âœ… |
| Cost/Problem | $0.03 | $9.64 | **320x** âŒ |

## ğŸš€ Next Steps

1. Build IMO reasoning pipeline (solve â†’ verify â†’ correct)
2. Test on math problems (100x cheaper)
3. Apply reasoning to SWE-bench (expect 33% â†’ 50%+)
